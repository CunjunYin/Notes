# Ensemble Learning

### what is Ensemble Learning

Suppose that you are seeking the answer to a complex question. You could ask the question to thousands of random people and then aggregate their answers. In many cases you will find that this aggregated answer is better than the answer from a single expert - `wisdom of the crowd`.

Ensemble learning is based on this concept

So if you gace a a group of predictors, if you aggregate the predictions of all the predictors together, you will often get better predictions than with the best individual predictor.

    A group of predictors is called an ensemble.

Those model should be uncorrelated as ensemble predictions can make better prodiction. `The reason for this wonderful effect is that the trees protect each other from their individual errors`

## Voting Classifiers
Suppose that you have trained a number of classi* fiers, each one achieving about 80% accuracy.

You can aggregate the predictions of the classifiers and predict the class by `hard voting` or `soft voting`.

* `hard voting classifier` - predicted class is the class that receives the highest vote.
* `soft voting classifier` - predicted class is the class that receives the highest average class probability.

Surprisingly, this voting classifier often achieves a higher accuracy than the best classifier in the ensemble. In fact, even if each
classifier is a weak learner. That is, the ensemble can be a strong learner, provided that there are enough weak learners and they are suffciently `diverse`. 

    Diversity - classifiers are perfectly independent and making uncorrelated errors.
    
One way to get diverse classifiers is to train them using very different algorithms.

## Bagging and Pasting
As mentioned above to get a diverse set of classifiers we should use very different training algorithms. Another approach is to use the same training algorithm for every classifiers but to train each classifiers on dierent random subsets of the training set.

        Draw random samples from the training set
    
There are `two` sampling methods:
1. Sampling with replacement - this method is called `bagging`
2. Sampling without replacement - this method is called `pasting`

Once all the predictors are trained, the ensemble can make a prediction for a new instance by aggregating the predictions from all the predictors. Typically, the statistical mode (i.e., the most frequent prediction) is used as the aggregation function.


Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and
variance.

In general, the net result is: the ensemble has a similar bias but a lower variance than a single predictor trained on the original whole
training set.

Bagging and pasting are popular methods because they scale very well: 
* the predictors can be trained in parallel, using dierent CPU
* cores or even on dierent servers. the predictors can also be tested in parallel.

## Bagging VS Pasting
Bagging introduces a bit more diversity in the subsets that each predictor is trained on, so slightly higher bias than pasting. However, it also means the predictors of bagging are less correlated, so lower variance
* bagging - lower variance
* pasting - lower bias

So, in ML we perfer lower variance, bagging is generally more preferred

![Sensitive to Scale](Assets/BaggingVSPasting.jpeg "Sensitive to Scale")

## Out-of-Bag Evaluation
With bagging, some instances may be sampled several times while others may not be sampled at all. On average, only about 63% of the training
instances are sampled for each predictor. The remaining 37% that
are not sampled are called out-of-bag (oob) instances.

> Note: they are not the same 37% for all predictors.

Since a predictor never sees the oob instances during training, we can use these instances to form our validation set for cross validation.

## `Random Patches` and `Random Subspaces`
`BaggingClassifier` has two fundamental hyperparameters
1. `max samples`, for controlling the maximum number of samples for each classifier in the ensemble; and
2. `bootstrap`, for specifying whether to use bagging or pasting.


`Sampling both` training `instances` *and* `features` is called the Random Patches method

`Keeping` `all` training `instances` *but* `sampling` `features` called the Random Subspaces method.

    Generally Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance.

> Rule of thumb: large dataset having many (suffcient) instances, then try Random Patches; otherwise, try Random Subspaces.

